{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1l_7yyznhLxoHCCrsG7KvfHZUaxd5hp_o","timestamp":1671426339315},{"file_id":"1nKECfGNgaa_ONaIflbEGHt9XhPvKg9qN","timestamp":1671380931095},{"file_id":"1cCpSHJg5keVp5hyX3SaIqFabdvAXj-5L","timestamp":1671374900602},{"file_id":"1gDZ2xcTOgR39tGGs-EZ6i3RTs16wmzZQ","timestamp":1666760889983},{"file_id":"https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb","timestamp":1591755516488}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"5h-5S_nveWpo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685171999338,"user_tz":-300,"elapsed":25630,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}},"outputId":"2a12313b-a2a7-4150-aa2a-9f11166a76a1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# clone YOLOv5 repository\n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","!git reset --hard fbe67e465375231474a2ad80a4389efc77ecff99"],"metadata":{"id":"Mse992jleZrx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685172001453,"user_tz":-300,"elapsed":2127,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}},"outputId":"47af625e-8b51-4586-b4bb-deb0df63d904"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 15705, done.\u001b[K\n","remote: Counting objects: 100% (33/33), done.\u001b[K\n","remote: Compressing objects: 100% (29/29), done.\u001b[K\n","remote: Total 15705 (delta 9), reused 14 (delta 4), pack-reused 15672\u001b[K\n","Receiving objects: 100% (15705/15705), 14.51 MiB | 29.12 MiB/s, done.\n","Resolving deltas: 100% (10750/10750), done.\n","/content/yolov5\n","HEAD is now at fbe67e4 Fix `OMP_NUM_THREADS=1` for macOS (#8624)\n"]}]},{"cell_type":"code","metadata":{"id":"wbvMlHd_QwMG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685172010955,"user_tz":-300,"elapsed":9507,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}},"outputId":"ba4ef9b0-9a14-4c3e-a4d9-1640cea92758"},"source":["# install dependencies as necessary\n","!pip install -qr requirements.txt  # install dependencies (ignore errors)\n","import torch\n","\n","from IPython.display import Image, clear_output  # to display images\n","from utils.downloads import attempt_download  # to download models/datasets\n","\n","# clear_output()\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.4/1.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hSetup complete. Using torch 2.0.1+cu118 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15101MB, multi_processor_count=40)\n"]}]},{"cell_type":"code","metadata":{"id":"Knxi2ncxWffW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685172120878,"user_tz":-300,"elapsed":40929,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}},"outputId":"541b2b09-b3e9-4562-ab36-d0e76700a226"},"source":["!pip install roboflow\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"zkyt15pyUkXSH9tlso0C\")\n","project = rf.workspace(\"noor-djz41\").project(\"multiuav\")\n","dataset = project.version(4).download(\"yolov5\")"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.0.9)\n","Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2022.12.7)\n","Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n","Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n","Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.22.4)\n","Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.7.0.72)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (8.4.0)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.4.7)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.27.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.15)\n","Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.2)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.65.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.0.7)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.39.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (2.0.12)\n","loading Roboflow workspace...\n","loading Roboflow project...\n","Downloading Dataset Version Zip in MultiUav-4 to yolov5pytorch: 100% [117629105 / 117629105] bytes\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Dataset Version Zip to MultiUav-4 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19773/19773 [00:04<00:00, 4837.23it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"dOPn9wjOAwwK","executionInfo":{"status":"ok","timestamp":1685172120878,"user_tz":-300,"elapsed":14,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"source":["# define number of classes based on YAML\n","import yaml\n","with open(dataset.location + \"/data.yaml\", 'r') as stream:\n","    num_classes = str(yaml.safe_load(stream)['nc'])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Rvt5wilnDyX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685172120879,"user_tz":-300,"elapsed":13,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}},"outputId":"84addf09-b6d4-4c5c-91a6-505e98f1726f"},"source":["#this is the model configuration we will use for our tutorial \n","%cat /content/yolov5/models/hub/yolov5s-transformer.yaml"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n","\n","# Parameters\n","nc: 80  # number of classes\n","depth_multiple: 0.33  # model depth multiple\n","width_multiple: 0.50  # layer channel multiple\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","\n","# YOLOv5 v6.0 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, C3, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 6, C3, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, C3, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 3, C3TR, [1024]],  # 9 <--- C3TR() Transformer module\n","   [-1, 1, SPPF, [1024, 5]],  # 9\n","  ]\n","\n","# YOLOv5 v6.0 head\n","head:\n","  [[-1, 1, Conv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, C3, [512, False]],  # 13\n","\n","   [-1, 1, Conv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, Conv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, Conv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]\n"]}]},{"cell_type":"code","metadata":{"id":"t14hhyqdmw6O","executionInfo":{"status":"ok","timestamp":1685172121493,"user_tz":-300,"elapsed":621,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"source":["#customize iPython writefile so we can write variables\n","from IPython.core.magic import register_line_cell_magic\n","\n","@register_line_cell_magic\n","def writetemplate(line, cell):\n","    with open(line, 'w') as f:\n","        f.write(cell.format(**globals()))"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n","   [-1, 1, CrossConv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, C3x, [128]],\n","   [-1, 1, CrossConv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 6, C3x, [256]],\n","   [-1, 1, CrossConv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, C3x, [512]],\n","   [-1, 1, CrossConv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 3, C3TR, [1024]],  # 9 <--- C3TR() Transformer module\n","   [-1, 1, SPPF, [1024, 5]],  # 9\n","  ]"],"metadata":{"id":"vYIKvoSzOiMA"}},{"cell_type":"code","metadata":{"id":"uDxebz13RdRA","executionInfo":{"status":"ok","timestamp":1685174518125,"user_tz":-300,"elapsed":362,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"source":["%%writetemplate /content/yolov5/models/YOLOv5-CVIT-PANet.yaml\n","\n","# parameters\n","nc: {num_classes}  # number of classes\n","depth_multiple: 0.33  # model depth multiple\n","width_multiple: 0.50  # layer channel multiple\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","# YOLOv5 v6.0 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, C3, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 6, C3, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, C3, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 3, C3TR, [1024]],  # 9 <--- C3TR() Transformer module\n","   [-1, 1, SPPF, [1024, 5]],  # 9\n","  ]\n","\n","# YOLOv5 v6.0 head\n","head:\n","  [[-1, 1, CrossConv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, C3x, [512, False]],  # 13\n","\n","   [-1, 1, CrossConv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, C3x, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, CrossConv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, C3x, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, CrossConv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, C3x, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","source":["%%time\n","%cd /content/yolov5/\n","##\n","!python train.py --img 416 --batch-size -1 --optimizer AdamW --cos-lr --label-smoothing 0.01  --patience 50  --epochs 1000 --hyp data/hyps/hyp.scratch-high.yaml --data {dataset.location}/data.yaml --cfg ./models/YOLOv5-CVIT-PANet.yaml --weights '' --project \"/content/gdrive/MyDrive/Runs\" --name YOLOv5-CVIT-PANet_results  --cache"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lxq-HQC5Ikfr","executionInfo":{"status":"ok","timestamp":1685175089321,"user_tz":-300,"elapsed":562856,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}},"outputId":"68b632ef-9b05-48f6-c871-5775f36f9df6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov5\n","2023-05-27 07:59:19.841931: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-27 07:59:20.759318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=./models/YOLOv5-CVIT-PANet.yaml, data=/content/yolov5/MultiUav-4/data.yaml, hyp=data/hyps/hyp.scratch-high.yaml, epochs=1000, batch_size=-1, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=AdamW, sync_bn=False, workers=8, project=/content/gdrive/MyDrive/Runs, name=YOLOv5-CVIT-PANet_results, exist_ok=False, quad=False, cos_lr=True, label_smoothing=0.01, patience=50, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mâš ï¸ YOLOv5 is out of date by 536 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n","YOLOv5 ðŸš€ v6.1-306-gfbe67e4 Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.1, copy_paste=0.1\n","\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/gdrive/MyDrive/Runs', view at http://localhost:6006/\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182976  models.common.C3TR                      [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    197632  models.common.CrossConv                 [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    296448  models.common.C3x                       [512, 256, 1, False]          \n"," 14                -1  1     49664  models.common.CrossConv                 [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     74496  models.common.C3x                       [256, 128, 1, False]          \n"," 18                -1  1     98816  models.common.CrossConv                 [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    230912  models.common.C3x                       [256, 256, 1, False]          \n"," 21                -1  1    394240  models.common.CrossConv                 [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1    920576  models.common.C3x                       [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","YOLOv5-CVIT-PANet summary: 291 layers, 6456072 parameters, 6456072 gradients, 15.0 GFLOPs\n","\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for --imgsz 416\n","\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (Tesla T4) 14.75G total, 0.32G reserved, 0.08G allocated, 14.34G free\n","      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n","     6456072       6.348         0.411         21.82         66.59        (1, 3, 416, 416)                    list\n","     6456072        12.7         0.294         17.73         42.15        (2, 3, 416, 416)                    list\n","     6456072       25.39         0.453         18.26         46.63        (4, 3, 416, 416)                    list\n","     6456072       50.78         0.763         21.91         49.04        (8, 3, 416, 416)                    list\n","     6456072       101.6         1.502         29.41         55.89       (16, 3, 416, 416)                    list\n","\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 161 for CUDA:0 12.88G/14.75G (87%) âœ…\n","Scaled weight_decay = 0.0012578125\n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW with parameter groups 59 weight (no decay), 69 weight, 64 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolov5/MultiUav-4/train/labels.cache' images and labels... 9461 found, 0 missing, 0 empty, 0 corrupt: 100% 9461/9461 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (4.9GB ram): 100% 9461/9461 [00:25<00:00, 375.31it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov5/MultiUav-4/valid/labels.cache' images and labels... 421 found, 0 missing, 0 empty, 0 corrupt: 100% 421/421 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB ram): 100% 421/421 [00:03<00:00, 135.04it/s]\n","Plotting labels to /content/gdrive/MyDrive/Runs/YOLOv5-CVIT-PANet_results5/labels.jpg... \n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.69 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n","Image sizes 416 train, 416 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1m/content/gdrive/MyDrive/Runs/YOLOv5-CVIT-PANet_results5\u001b[0m\n","Starting training for 1000 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     0/999     14.7G   0.09284   0.01478   0.02138       325       416: 100% 59/59 [01:14<00:00,  1.26s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:04<00:00,  2.22s/it]\n","                 all        421        486     0.0106      0.309      0.031    0.00852\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     1/999     13.2G   0.07679   0.01413   0.02066       321       416: 100% 59/59 [01:08<00:00,  1.16s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:01<00:00,  1.84it/s]\n","                 all        421          0          0          0          0          0\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     2/999     13.2G       nan       nan       nan       286       416: 100% 59/59 [01:11<00:00,  1.22s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:01<00:00,  1.79it/s]\n","                 all        421          0          0          0          0          0\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     3/999     13.2G       nan       nan       nan       322       416: 100% 59/59 [01:09<00:00,  1.18s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:01<00:00,  1.49it/s]\n","                 all        421          0          0          0          0          0\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     4/999     13.2G       nan       nan       nan       330       416: 100% 59/59 [01:10<00:00,  1.19s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:01<00:00,  1.51it/s]\n","                 all        421          0          0          0          0          0\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     5/999     13.2G       nan       nan       nan       309       416: 100% 59/59 [01:08<00:00,  1.16s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:01<00:00,  1.82it/s]\n","                 all        421          0          0          0          0          0\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     6/999     13.2G       nan       nan       nan       393       416:  90% 53/59 [01:04<00:07,  1.21s/it]\n","Traceback (most recent call last):\n","  File \"/content/yolov5/train.py\", line 642, in <module>\n","    main(opt)\n","  File \"/content/yolov5/train.py\", line 537, in main\n","    train(opt.hyp, opt, device, callbacks)\n","  File \"/content/yolov5/train.py\", line 339, in train\n","    scaler.step(optimizer)  # optimizer.step\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 374, in step\n","    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 289, in _maybe_opt_step\n","    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 289, in <genexpr>\n","    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n","KeyboardInterrupt\n","^C\n","CPU times: user 4.15 s, sys: 507 ms, total: 4.65 s\n","Wall time: 9min 22s\n"]}]},{"cell_type":"code","source":["#Later, if it is stopped prematurely and I need to resume, I use\n","!python train.py --img 416 --weights '' --project \"/content/gdrive/MyDrive/Runs\"  --name YOLOv5-CVIT-PANet_results  --cache  --resume \"/content/gdrive/MyDrive/Runs/YOLOv5-CVIT-PANet_results/weights/last.pt\""],"metadata":{"id":"FtZjt2NHChUq","executionInfo":{"status":"aborted","timestamp":1685172022836,"user_tz":-300,"elapsed":45,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOy5KI2ncnWd","executionInfo":{"status":"aborted","timestamp":1685172022837,"user_tz":-300,"elapsed":45,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"source":["# Start tensorboard\n","# Launch after you have started training\n","# logs save in the folder \"runs\"\n","%load_ext tensorboard\n","%tensorboard --logdir runs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C60XAsyv6OPe","executionInfo":{"status":"aborted","timestamp":1685172022837,"user_tz":-300,"elapsed":45,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"source":["# we can also output some older school graphs if the tensor board isn't working for whatever reason... \n","from utils.plots import plot_results  # plot results.txt as results.png\n","Image(filename='/content/yolov5/runs/train/YOLOv5-CVIT-PANet/results.png', width=1000)  # view results.png"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIEwt5YLeQ7P","executionInfo":{"status":"aborted","timestamp":1685172022838,"user_tz":-300,"elapsed":46,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"source":["# trained weights are saved by default in our weights folder\n","%ls runs/\n","%ls runs/train/YOLOv5-CVIT-PANet_results/weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#########Validate whole dataset\n","# Validate model and print evaluation metrics\n","!python val.py --weights runs/train/YOLOv5-CVIT-PANet_results/weights/best.pt --data {dataset.location}/data.yaml --img 256 --half"],"metadata":{"id":"l7zTkpkEmFBO","executionInfo":{"status":"aborted","timestamp":1685172022840,"user_tz":-300,"elapsed":47,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9nmZZnWOgJ2S","executionInfo":{"status":"aborted","timestamp":1685172022843,"user_tz":-300,"elapsed":50,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"source":["######## Test Valid image folder\n","%cd /content/yolov5/\n","!python detect.py --weights runs/train/YOLOv5-CVIT-PANet_results/weights/best.pt --img 320 --conf 0.25 --source /content/yolov5/MultiUav-3/valid/images"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python val.py --task study --weights runs/train/YOLOv5-CVIT-PANet_results/weights/best.pt--data {dataset.location}/data.yaml --iou 0.5 "],"metadata":{"id":"wajSppXReCVQ","executionInfo":{"status":"aborted","timestamp":1685172022845,"user_tz":-300,"elapsed":52,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python val.py --task speed --weights runs/train/YOLOv5-CVIT-PANet_results/weights/best.pt --data {dataset.location}/data.yaml --iou 0.5 "],"metadata":{"id":"9Gj5SHU3eFOL","executionInfo":{"status":"aborted","timestamp":1685172022845,"user_tz":-300,"elapsed":52,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##I have created a zip file:\n","!zip -r /content/YOLOv5-CVIT.zip /content/yolov5/runs/\n","##Than I have downloded that zip file:\n","from google.colab import files\n","files.download(\"/content/YOLOv5-CVIT.zip\")"],"metadata":{"id":"qnc-LU-nuboL","executionInfo":{"status":"aborted","timestamp":1685172022846,"user_tz":-300,"elapsed":53,"user":{"displayName":"misha uroojkhan","userId":"10852817853753218320"}}},"execution_count":null,"outputs":[]}]}